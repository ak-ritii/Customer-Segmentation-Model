# -*- coding: utf-8 -*-
"""Customer Segmentation - Akriti Verma.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1TcX5PyOjV9sED3umBlAfOG1NCJUBeQUD

#Importing the Libraries
"""

import numpy as np  #Numerical Python -> Mathematical Operation
import pandas as pd #Data Manipulation
import matplotlib.pyplot as plt #Data Visualization
import seaborn as sns #Data Visualization
import re  #Data CLeaning
from sklearn.cluster import KMeans #Model 
from sklearn.preprocessing import LabelEncoder,MinMaxScaler,StandardScaler #Converting the categorical data into numerical data

"""#Importing The Dataset

Link-https://drive.google.com/file/d/1g5j9WzJBdGwzCLOBXKf3-kp8bvazVvc-/view
"""

df=pd.read_csv("/content/drive/MyDrive/CodeClause/marketing_campaign.csv",sep="\t")

"""#Exploratoy Data Analysis"""

df.head() #Viewing the first 5 datapoints

df.tail() #Viewing the last 5 data points

df.shape #Rows and Columns

df.info() #Overview of dataset

df.describe() #Statistical Description

df.isna() #Null values

df.isna().sum() #sum of null values

df.isna().sum().sum() #total Null values in dataset

df.duplicated().sum() #Sum of Duplicate Values in dataset

df.head(2) #First two rows of dataset

df.drop("ID",axis=1,inplace=True) #Deleting Id Column

df.head(1) #Showing the 1st datapoint

import datetime #Importing datetime

df["Age"]=datetime.date.today().year-df["Year_Birth"] #Present Age Calculation

df["Age"] #Age Column

df.drop("Year_Birth",axis=1,inplace=True) #dropping Year Birth Column

df.Education.value_counts() #unique valurs of education column

#UnderGraduate->Graduation,2n Cycle,Basic
#Graduate->Master,PhD

df.Education.replace(["Graduation","PhD","Master","2n Cycle","Basic"],["UnderGraduate","Graduate","Graduate","UnderGraduate","UnderGraduate"],inplace=True) #Classifying values into two sets

df.Education.value_counts() #Unique Values in Education

le=LabelEncoder() #For categorizing data into numerical values

df.Education=le.fit_transform(df.Education) #Chaning value into numerical class

df.Education.value_counts() #Unique Values

df.head(1) #First row of dataset

df.Marital_Status.value_counts() #Unique values in Marital_Status

#Married-Married,Together
#Unmarried-Single,Divorced,Widow,Alone,Absurd,YOLO


#Classifying Categories into 2
df.Marital_Status.replace(["Married","Together","Single","Divorced","Widow","Alone","Absurd","YOLO"],["Married","Married","UnMarried","UnMarried","UnMarried","UnMarried","UnMarried","UnMarried"],inplace=True)

df.Marital_Status.value_counts() #Unique categories

df.Marital_Status=le.fit_transform(df.Marital_Status) #Converting Categorical data into numerical

df.Marital_Status.value_counts() #Unique Category

df.head(1) #1st row

df.Income.mean() #mean of income

df.Income.median() #medain of Income

df.Income.mode() #mode of Income

df.Income.describe() #description of Income

df[["Income"]].boxplot() #Visualizing Outliers

df.Income.fillna(df.Income.median(),inplace=True) #filling null values with median

df.Income.isna().sum() #Checking count of null values

df[["Income"]].boxplot() #Visualizing Income

in_mean=df.Income.mean()
in_std=df.Income.std()
threshold=3
outliers=[] ##creating outliers list

for i in df.Income:
  z=(i-in_mean)/in_std
  if z>threshold:
    outliers.append(i) #appending values in list

outliers #outliers in income

df[["Age"]].boxplot() #visualizing age

age_mean=df.Age.mean()
age_std=df.Age.std()
threshold=3
age_outliers=[] 3creating age_outliers list

for i in df.Age:
  z=(i-age_mean)/age_std
  if z>threshold:
    age_outliers.append(i) #appending outliers in list

age_outliers #outliers in age column

int(df.Age.median()) #Integer median

df.Age.replace(age_outliers,[int(df.Age.median()),int(df.Age.median()),int(df.Age.median())],inplace=True) #replacing outliers with median

df[["Age"]].boxplot() #visualization of age column

df.head(1) #1st row

#ChildrenHome=KidHome=+TeenHome

df["ChildrenHome"]=df.Kidhome+df.Teenhome #combing two columns into one

df["ChildrenHome"] #all values of Childrenhome

df.drop(["Kidhome","Teenhome"],axis=1,inplace=True) #dropping 2 columns

df.head(1) #1st row

type(df.Dt_Customer[9]) #type of column

pd.DatetimeIndex(df["Dt_Customer"]).year #Present year in column

df.Dt_Customer=datetime.date.today().year-pd.DatetimeIndex(df["Dt_Customer"]).year #Total time

df.head() #top 5 rows

df.columns #all column names

df["Spent"]=df["MntWines"]+df["MntFruits"]+df["MntMeatProducts"]+df["MntFishProducts"]+df["MntSweetProducts"]+df["MntGoldProds"] #new column

df.drop(["MntWines","MntFruits","MntMeatProducts","MntFishProducts","MntSweetProducts","MntGoldProds"],axis=1,inplace=True) #Dropping 8 columns

df.head(1) #1st row

df.columns #All column names

df["Purchase"]=df["NumDealsPurchases"]+df["NumWebPurchases"]+df["NumCatalogPurchases"]+df["NumStorePurchases"]+df["NumWebVisitsMonth"] #new column

df.drop(["NumDealsPurchases","NumWebPurchases","NumCatalogPurchases","NumStorePurchases","NumWebVisitsMonth"],axis=1,inplace=True) #Dropping 5 Columns

df.head(1) #1st row

df.drop(["Z_CostContact","Z_Revenue"],axis=1,inplace=True) #Dropping 2 columns

df.info() #Information of dataset

"""#Feature Scaling

"""

scaler=MinMaxScaler() #to convert all values between 0 and 1

scaler_df=scaler.fit_transform(df) #coverting adatset into 0 ans 1

scaler_df[0] #values in 1st row

df.iloc[0]

ssd=[]
for i in range(1,11):
  Kmodel=KMeans(n_clusters=i,n_init=15,max_iter=500)
  Kmodel.fit(df)
  ssd.append(Kmodel.inertia_) #Inertia indicates all the errors

plt.plot(range(1,11),ssd,marker="o")
plt.grid()
plt.title("Elbow Plot")

#k=6 We are taking the value of k=6

Kmodel=KMeans(n_clusters=6) #clusters

Kmodel.fit(df) #fitting model

predict=Kmodel.predict(df) #modeling

predict[:10] #prediction in 10 rows

df["Cluster"]=predict #prediction

df.head() #1st 5 row

from scipy.cluster.hierarchy import linkage,dendrogram #Importing linkage and dendrogram to represent hierarchial clustering

var=linkage(df,method="ward") #method of linkage

plt.figure(figsize=(70,50))
dendrogram(var,leaf_rotation=90) #visualizing